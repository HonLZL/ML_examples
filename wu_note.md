### 3 分类

#### 3.1 逻辑回归

==sigmoid函数== 
$$
g(z)=\frac{1}{1+e^{-z}} \ \ \ \  \ g\in(0, 1)
$$
输入时 z ，z 也可以时多个变量的函数。 g 能表示**概率**。

![image-20230211151049638](wu_note_imgs\image-20230211151049638.png)



决策边界 decision boundaries

有线性边界和非线性边界，令 z = 0 可以实现推导。

| <img src="wu_note_imgs\image-20230211154829261.png" alt="image-20230211154829261" style="zoom:50%;" /> | <img src="wu_note_imgs\image-20230211154942251.png" alt="image-20230211154942251" style="zoom:50%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

代价函数

舍弃平方误差，因为这样的损失函数不是凸函数。

选择加上log，这样是凸函数，从而可以使用梯度下降了。

![image-20230211162450516](wu_note_imgs\image-20230211162450516.png)

化简，求和得到代价函数 J 

<img src="wu_note_imgs\image-20230211163812879.png" alt="image-20230211163812879" style="zoom:67%;" />



梯度下降的直观理解

![image-20230211170944265](wu_note_imgs\image-20230211170944265.png)

将梯度下降应用到逻辑回归

![image-20230211165110910](wu_note_imgs\image-20230211165110910.png)



#### 3.2 过拟合

过拟合是指算法过于依赖训练样本自身特点导致泛化性能下降的问题。

欠拟合是指算法没能学习到训练样本的通用特点导致拟合效果不好，一般是由于训练数据少或者算法学习能力差导致的。

**解决过拟合** 

1. 收集更多数据
2. 去除一些影响小的特征
3. 正则化 regularization

正则化是调整特征权重的过程，减小 w ：

![image-20230211172607590](wu_note_imgs\image-20230211172607590.png)

具体做法，在计算代价函数 J 时，$J'=J+1000w_i$ 这样会使 $w_i$ 更小。

在实际使用正则化时，通常引入正则化参数 $\lambda$， 如下图：

![image-20230211192655561](wu_note_imgs\image-20230211192655561.png)



**线性回归的正则化** 

求导时跟随着正则项，b是不需要正则化的。

![image-20230211193107124](wu_note_imgs\image-20230211193107124.png)

过拟合的时候，拟合函数的系数往往非常大，为什么？过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。

而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。



**逻辑回归的正则化** 

与线性回归类似。

![image-20230211194710000](wu_note_imgs\image-20230211194710000.png)



W_j 可以转化为，$(1-\alpha\frac{\lambda}{m})$ 是小于 1 的正数，说明正则化是对 w 的缩小。 

<img src="wu_note_imgs\image-20230212214347670.png" alt="image-20230212214347670" style="zoom:67%;" />











